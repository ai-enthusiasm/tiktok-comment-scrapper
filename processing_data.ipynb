{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import libraries such as pandas, numpy, matplotlib, seaborn, tensorflow/keras, and json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 12:51:34.453713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744030294.499436  885313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744030294.511921  885313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744030294.590900  885313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744030294.590937  885313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744030294.590939  885313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744030294.590941  885313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 12:51:34.602098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data processing, visualization, and deep learning\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns  # For enhanced data visualization\n",
    "import tensorflow as tf  # For building and training deep learning models\n",
    "from tensorflow import keras  # High-level API for TensorFlow\n",
    "import json  # For working with JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore JSON Data\n",
    "Load all `.json` files from the folder, combine them into a single DataFrame, and explore the structure and content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "                                             caption  \\\n",
      "0  PHÁO ĐÃ LÊN NÒNG, vào livestream đối chất 1:1 ...   \n",
      "1  tư duy ngược - một phương pháp học rất quen th...   \n",
      "2  Chúc các bạn 2k6 thật nhiều may mắn, bình tâm ...   \n",
      "3  Nhật ký ôn thi THPTQG| Chúng ta sẽ không có cơ...   \n",
      "4  [D51] “Tôi không có thiên phú, nhưng tôi muốn ...   \n",
      "\n",
      "                                           video_url  \\\n",
      "0  https://t.tiktok.com/i18n/share/video/74868857...   \n",
      "1  https://t.tiktok.com/i18n/share/video/73402265...   \n",
      "2  https://t.tiktok.com/i18n/share/video/73766570...   \n",
      "3  https://t.tiktok.com/i18n/share/video/71985216...   \n",
      "4  https://t.tiktok.com/i18n/share/video/73658524...   \n",
      "\n",
      "                                            comments  has_more  \n",
      "0  [{'comment_id': '7486886369214874369', 'userna...         1  \n",
      "1  [{'comment_id': '7341636945293034247', 'userna...         1  \n",
      "2  [{'comment_id': '7486681879690117895', 'userna...         1  \n",
      "3  [{'comment_id': '7198741785848709915', 'userna...         1  \n",
      "4  [{'comment_id': '7453156872272085776', 'userna...         1  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25 entries, 0 to 24\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   caption    25 non-null     object\n",
      " 1   video_url  25 non-null     object\n",
      " 2   comments   25 non-null     object\n",
      " 3   has_more   25 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 932.0+ bytes\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "                                                  caption  \\\n",
      "count                                                  25   \n",
      "unique                                                 25   \n",
      "top     PHÁO ĐÃ LÊN NÒNG, vào livestream đối chất 1:1 ...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                                                video_url  \\\n",
      "count                                                  25   \n",
      "unique                                                 25   \n",
      "top     https://t.tiktok.com/i18n/share/video/74868857...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "                                                 comments  has_more  \n",
      "count                                                  25      25.0  \n",
      "unique                                                 25       NaN  \n",
      "top     [{'comment_id': '7486886369214874369', 'userna...       NaN  \n",
      "freq                                                    1       NaN  \n",
      "mean                                                  NaN       1.0  \n",
      "std                                                   NaN       0.0  \n",
      "min                                                   NaN       1.0  \n",
      "25%                                                   NaN       1.0  \n",
      "50%                                                   NaN       1.0  \n",
      "75%                                                   NaN       1.0  \n",
      "max                                                   NaN       1.0  \n"
     ]
    }
   ],
   "source": [
    "import os  # For interacting with the file system\n",
    "\n",
    "# Define the folder path containing the JSON files\n",
    "folder_path = '/home/nhatlinh/tiktok-comment-scrapper'\n",
    "\n",
    "# List all JSON files in the folder\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store data from all JSON files\n",
    "data_list = []\n",
    "\n",
    "# Load each JSON file and append its content to the data list\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        data_list.append(data)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the first few rows of the DataFrame to explore its structure\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics of the DataFrame\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "Clean the text data by removing special characters, converting to lowercase, and handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Before Cleaning:\n",
      "caption      0\n",
      "video_url    0\n",
      "comments     0\n",
      "has_more     0\n",
      "dtype: int64\n",
      "\n",
      "First few rows after cleaning:\n",
      "                                            comments  \\\n",
      "0  {'comment_id': '7483899822577320711', 'usernam...   \n",
      "1  {'comment_id': '7483912469787280149', 'usernam...   \n",
      "2  {'comment_id': '7484401968984064786', 'usernam...   \n",
      "3  {'comment_id': '7483964387545760520', 'usernam...   \n",
      "4  {'comment_id': '7483914010791822098', 'usernam...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  [đâu phải t nhiên tên là viruss, đúng là cái t...  \n",
      "1  [ổng nói gì mẹ ngọc kem v ạ, tui cx tò mò nx, ...  \n",
      "2  [mà này vr đã nói gì về mẹ của nk vậy mn tại t...  \n",
      "3  [giờ viruss lên bài đính chính tôi là bot may ...  \n",
      "4  [rep may mắn cả đời, rep, , rep, , rep, , , , ...  \n",
      "\n",
      "Missing Values After Cleaning:\n",
      "caption         0\n",
      "video_url       0\n",
      "comments        0\n",
      "has_more        0\n",
      "cleaned_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Load data from the JSON file\n",
    "json_file_path = \"/home/nhatlinh/tiktok-comment-scrapper/neg7483896983124462856.json\"\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert JSON data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check for missing values in the DataFrame\n",
    "print(\"\\nMissing Values Before Cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in the 'comments' column\n",
    "if 'comments' in df.columns:\n",
    "    df = df.dropna(subset=['comments'])\n",
    "else:\n",
    "    raise KeyError(\"The 'comments' column is missing in the DataFrame.\")\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_comment_text(data):\n",
    "    \"\"\"\n",
    "    Trích xuất và làm sạch phần 'comment' từ dữ liệu TikTok (bao gồm cả reply).\n",
    "    - Chỉ giữ lại comment (không quan tâm username, avatar, v.v.)\n",
    "    - Loại bỏ emoji, ký tự đặc biệt, số\n",
    "    - Giữ lại chữ cái tiếng Việt (kể cả có dấu)\n",
    "    - Chuyển về chữ thường\n",
    "    - Xóa khoảng trắng thừa\n",
    "    \"\"\"\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"[^a-zA-ZÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểếỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỳỵỷỹÝýỴỶỸ\\s]\", '', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    result = []\n",
    "    # Làm sạch comment chính\n",
    "    if 'comment' in data:\n",
    "        result.append(clean_text(data['comment']))\n",
    "    # Làm sạch comment trong replies (nếu có)\n",
    "    for reply in data.get('replies', []):\n",
    "        if 'comment' in reply:\n",
    "            result.append(clean_text(reply['comment']))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the 'comments' column\n",
    "df['cleaned_text'] = df['comments'].apply(clean_comment_text)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "print(\"\\nFirst few rows after cleaning:\")\n",
    "print(df[['comments', 'cleaned_text']].head())\n",
    "\n",
    "# Check for missing values after cleaning\n",
    "print(\"\\nMissing Values After Cleaning:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhìn chị như muốn khóc vậy á',\n",
       " 'khóc mấy ngày nay rồi á tr thương bả lắm luônn',\n",
       " 'thương thật']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Data for Sentiment Classification\n",
    "Assign sentiment labels (neg, neutral, pos) based on the file names or content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (25) does not match length of index (1549)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a new column 'label' in the DataFrame by mapping file names to labels\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [assign_label(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m json_files]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the distribution of sentiment labels\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentiment Label Distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (25) does not match length of index (1549)"
     ]
    }
   ],
   "source": [
    "# Assign sentiment labels based on file names\n",
    "def assign_label(file_name):\n",
    "    if file_name.startswith('neg'):\n",
    "        return 'neg'\n",
    "    elif file_name.startswith('pos'):\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Create a new column 'label' in the DataFrame by mapping file names to labels\n",
    "df['label'] = [assign_label(file) for file in json_files]\n",
    "\n",
    "# Display the distribution of sentiment labels\n",
    "print(\"\\nSentiment Label Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize the distribution of sentiment labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='label', data=df, palette='viridis')\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data Insights\n",
    "Use matplotlib and seaborn to create visualizations such as bar plots for sentiment distribution and word clouds for common words in each sentiment category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for visualization\n",
    "from wordcloud import WordCloud  # For generating word clouds\n",
    "\n",
    "# Visualize the distribution of sentiment labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='label', data=df, palette='viridis')\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Generate word clouds for each sentiment category\n",
    "sentiments = ['neg', 'neutral', 'pos']\n",
    "for sentiment in sentiments:\n",
    "    # Filter the DataFrame for the current sentiment\n",
    "    sentiment_text = df[df['label'] == sentiment]['cleaned_text']\n",
    "    \n",
    "    # Combine all text for the current sentiment into a single string\n",
    "    combined_text = ' '.join(sentiment_text)\n",
    "    \n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {sentiment.capitalize()} Sentiment')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Model Training\n",
    "Tokenize and pad the text data, split it into training and validation sets, and encode the sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the text data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the tokenizer with a maximum number of words\n",
    "max_words = 10000  # Maximum number of words to keep in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "\n",
    "# Fit the tokenizer on the cleaned text data\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "# Convert the text data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "max_sequence_length = 100  # Maximum length of sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Encode the sentiment labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    ")\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train Deep Learning Model\n",
    "Define a deep learning model using TensorFlow/Keras, compile it, and train it on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the deep learning model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length),  # Embedding layer\n",
    "    LSTM(128, return_sequences=True),  # LSTM layer with 128 units\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    LSTM(64),  # Another LSTM layer with 64 units\n",
    "    Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    Dense(64, activation='relu'),  # Fully connected layer with ReLU activation\n",
    "    Dense(3, activation='softmax')  # Output layer with softmax activation for 3 sentiment classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    metrics=['accuracy']  # Metric to monitor\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32  # Number of samples per batch\n",
    "epochs = 10  # Number of epochs to train\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,  # Training data and labels\n",
    "    validation_data=(X_val, y_val),  # Validation data and labels\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1  # Display training progress\n",
    ")\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Evaluate the model on validation data and visualize metrics such as accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Generate predictions on the validation data\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Import classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
